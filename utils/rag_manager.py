"""
RAG (Retrieval-Augmented Generation) ì‹œìŠ¤í…œ
- FAISS ë²¡í„° ìŠ¤í† ì–´
- í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ (nlpai-lab/KoE5)
"""
import os
from typing import List, Dict, Optional
from pathlib import Path
import pickle

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.docstore.document import Document

from utils.pdf_processor import PatentPDFProcessor


class PatentRAGManager:
    """íŠ¹í—ˆ ë¬¸ì„œ RAG ê´€ë¦¬"""
    
    def __init__(
        self, 
        embedding_model: str = "nlpai-lab/KoE5",
        chunk_size: int = 1000,
        chunk_overlap: int = 200,
        index_path: str = "faiss_index"
    ):
        self.embedding_model = embedding_model
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.index_path = Path(index_path)
        
        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        print(f"ğŸ“¦ ì„ë² ë”© ëª¨ë¸ ë¡œë”©: {embedding_model}")
        self.embeddings = HuggingFaceEmbeddings(
            model_name=embedding_model,
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
        
        self.vector_store = None
        self.metadata_store = {}
        
    def build_from_pdfs(self, pdf_paths: List[str]) -> Dict:
        """PDF íŒŒì¼ë“¤ë¡œë¶€í„° RAG êµ¬ì¶•"""
        print(f"\nğŸ”¨ {len(pdf_paths)}ê°œ íŠ¹í—ˆ PDFë¡œ RAG êµ¬ì¶• ì‹œì‘...")
        
        all_documents = []
        
        for pdf_path in pdf_paths:
            try:
                print(f"  ì²˜ë¦¬ ì¤‘: {Path(pdf_path).name}")
                
                # PDF íŒŒì‹±
                processor = PatentPDFProcessor(pdf_path)
                metadata = processor.extract_metadata()
                chunks = processor.get_text_chunks(
                    chunk_size=self.chunk_size,
                    overlap=self.chunk_overlap
                )
                processor.close()
                
                # Document ê°ì²´ ìƒì„±
                for i, chunk in enumerate(chunks):
                    doc = Document(
                        page_content=chunk,
                        metadata={
                            "source": pdf_path,
                            "patent_number": metadata.get("number", "Unknown"),
                            "patent_title": metadata.get("title", "Unknown"),
                            "applicant": metadata.get("applicant", "Unknown"),
                            "chunk_id": i,
                            "total_chunks": len(chunks)
                        }
                    )
                    all_documents.append(doc)
                
                # ë©”íƒ€ë°ì´í„° ì €ì¥
                self.metadata_store[pdf_path] = metadata
                
                print(f"    âœ… ì²­í¬ ìˆ˜: {len(chunks)}")
                
            except Exception as e:
                print(f"    âŒ ì˜¤ë¥˜: {e}")
                continue
        
        # FAISS ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
        print(f"\nğŸ” FAISS ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì¤‘... (ì´ {len(all_documents)}ê°œ ì²­í¬)")
        self.vector_store = FAISS.from_documents(
            documents=all_documents,
            embedding=self.embeddings
        )
        
        print("âœ… RAG êµ¬ì¶• ì™„ë£Œ!\n")
        
        # âœ… ìˆ˜ì •: total_chunks í‚¤ ì¶”ê°€
        return {
            "total_documents": len(all_documents),
            "total_chunks": len(all_documents),  # âœ… ì¶”ê°€
            "total_patents": len(pdf_paths),
            "metadata": self.metadata_store
        }
    
    def search(
        self, 
        query: str, 
        k: int = 5,
        filter_patent: Optional[str] = None
    ) -> List[Document]:
        """ìœ ì‚¬ë„ ê²€ìƒ‰"""
        if not self.vector_store:
            raise ValueError("ë²¡í„° ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. build_from_pdfs()ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        
        # íŠ¹ì • íŠ¹í—ˆë¡œ í•„í„°ë§
        if filter_patent:
            # ë” ë§ì´ ê²€ìƒ‰í•˜ì—¬ í•„í„°ë§ í›„ kê°œ í™•ë³´
            results = self.vector_store.similarity_search(
                query, 
                k=k*3
            )
            # ë©”íƒ€ë°ì´í„° í•„í„°ë§
            filtered_results = [
                doc for doc in results 
                if filter_patent in doc.metadata.get("source", "")
            ]
            
            # í•„í„°ë§ ê²°ê³¼ ë°˜í™˜
            return filtered_results[:k]
        else:
            results = self.vector_store.similarity_search(query, k=k)
            return results
    
    def get_patent_summary(self, pdf_path: str, max_chunks: int = 10) -> str:
        """íŠ¹ì • íŠ¹í—ˆì˜ ìš”ì•½ ìƒì„± (ì²˜ìŒ Nê°œ ì²­í¬)"""
        results = self.search(
            query="ë°œëª…ì˜ ë‚´ìš© ë°°ê²½ê¸°ìˆ  í•´ê²°ê³¼ì œ",
            k=max_chunks,
            filter_patent=pdf_path
        )
        
        summary_parts = []
        for doc in results:
            summary_parts.append(doc.page_content)
        
        return "\n\n".join(summary_parts)
    
    def save_index(self):
        """ë²¡í„° ìŠ¤í† ì–´ ì €ì¥"""
        if not self.vector_store:
            raise ValueError("ì €ì¥í•  ë²¡í„° ìŠ¤í† ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        self.index_path.mkdir(exist_ok=True)
        
        # FAISS ì¸ë±ìŠ¤ ì €ì¥
        self.vector_store.save_local(str(self.index_path))
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata_path = self.index_path / "metadata.pkl"
        with open(metadata_path, "wb") as f:
            pickle.dump(self.metadata_store, f)
        
        print(f"ğŸ’¾ ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ: {self.index_path}")
    
    def load_index(self):
        """ì €ì¥ëœ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ"""
        if not self.index_path.exists():
            raise FileNotFoundError(f"ì¸ë±ìŠ¤ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {self.index_path}")
        
        # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
        self.vector_store = FAISS.load_local(
            str(self.index_path),
            self.embeddings,
            allow_dangerous_deserialization=True
        )
        
        # ë©”íƒ€ë°ì´í„° ë¡œë“œ
        metadata_path = self.index_path / "metadata.pkl"
        if metadata_path.exists():
            with open(metadata_path, "rb") as f:
                self.metadata_store = pickle.load(f)
        
        print(f"ğŸ“‚ ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ: {self.index_path}")


def create_rag_system(pdf_paths: List[str], save: bool = True) -> PatentRAGManager:
    """RAG ì‹œìŠ¤í…œ ìƒì„± í—¬í¼ í•¨ìˆ˜"""
    rag_manager = PatentRAGManager()
    
    # RAG êµ¬ì¶•
    build_info = rag_manager.build_from_pdfs(pdf_paths)
    
    print(f"ğŸ“Š êµ¬ì¶• ì •ë³´:")
    print(f"  - ì´ íŠ¹í—ˆ: {build_info['total_patents']}ê°œ")
    print(f"  - ì´ ì²­í¬: {build_info['total_documents']}ê°œ")
    
    # ì €ì¥
    if save:
        rag_manager.save_index()
    
    return rag_manager


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    test_pdfs = [
        "data/patent1.pdf",
        "data/patent2.pdf",
        "data/patent3.pdf"
    ]
    
    # RAG êµ¬ì¶•
    rag = create_rag_system(test_pdfs)
    
    # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    print("\nğŸ” ê²€ìƒ‰ í…ŒìŠ¤íŠ¸:")
    query = "LLM ê¸°ë°˜ ê³ ê° ìƒë‹´"
    results = rag.search(query, k=3)
    
    for i, doc in enumerate(results, 1):
        print(f"\n[ê²°ê³¼ {i}]")
        print(f"íŠ¹í—ˆ: {doc.metadata.get('patent_number')}")
        print(f"ë‚´ìš©: {doc.page_content[:200]}...")